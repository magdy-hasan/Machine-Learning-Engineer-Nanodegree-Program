{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data we saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = pickle.load(open('da10c.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the pretrained model VGG16 from keras, default training is on imagenet dataset\n",
    "### I also tried resnet50 and [keras vgface](https://github.com/rcmalli/keras-vggface) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here i used layer with output (7,7,512), i also tried with using layer fc2 with output 4096 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f50d7432d30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f50d76d1940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x7f50d7b9eef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Flatten at 0x7f50d8465710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'block5_pool/MaxPool:0' shape=(?, 7, 7, 512) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-1].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model.inputs, model.layers[-1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'path', 'ne_age'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remeber my data distrubtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f50d4e8ab00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "da['ne_age'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i also tried creating dicionary with each class weight from sklearn libary to use it in train keras model in loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cw = class_weight.compute_class_weight('balanced', np.unique(da['ne_age']), da['ne_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07389599, 0.87889902, 1.0830128 , 0.93650813, 1.01969838,\n",
       "       1.03436106, 0.91128603, 0.97793297, 1.04794502, 1.08902671])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dw = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(10):\n",
    "    #dw[i] = cw[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.073895987141417,\n",
       " 1: 0.8788990212035406,\n",
       " 2: 1.0830128029012582,\n",
       " 3: 0.9365081250593937,\n",
       " 4: 1.0196983806715298,\n",
       " 5: 1.0343610600892155,\n",
       " 6: 0.9112860346302333,\n",
       " 7: 0.977932967823563,\n",
       " 8: 1.0479450233943004,\n",
       " 9: 1.0890267149211261}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(dw,open('dw.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# this takes around 4 hours even on aws !\n",
    "X,Y = [],[]\n",
    "cnt = 0\n",
    "for i,r in da.iterrows():\n",
    "    path = 'imdb_crop/' + r['path']\n",
    "    # load image with keras image utils\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    img = image.img_to_array(img)\n",
    "    # reshape with batch_size for model\n",
    "    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "    # preprocess_input provided by keras, should take care of normizling image\n",
    "    img = preprocess_input(img)\n",
    "    feature = model.predict(img, verbose=0)[0]\n",
    "    X.append(feature)\n",
    "    # encode age prediction\n",
    "    y = to_categorical(r['ne_age'], num_classes=10)\n",
    "    Y.append(y)\n",
    "    # create training files each with 2048 data point, as dataset is too big to be loaded into memory\n",
    "    if len(X) % 2048 == 0:\n",
    "        cnt += 1\n",
    "        print(cnt)\n",
    "        x1 = np.array(X)\n",
    "        y1 = np.array(Y)\n",
    "        pickle.dump([x1,y1],open('trainfiles/train' + str(cnt) + '.pkl', 'wb'))\n",
    "        del X,Y,x1,y1\n",
    "        X,Y = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
